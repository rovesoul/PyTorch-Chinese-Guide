# 05 卷积神经网络
> 上个笔记中了解了 分类问题，回归问题，学习了多层神经网络结构的样子。还学习了反向传播算法等。但现实生活中很多问题他们解决不了。在神经网络优化过程中，出现了卷积神经网络。大体记录的一些发展节点如下：
- 上世纪机器识图，如手写邮编如何识别呢？
- 上世纪60年代观察猫，发现视觉神经有一小部分的响应，引起思考
- 上世纪80年代反向传播，卷积神经网络开始流行，
- 1998年，LeCun提出LeNet-5MNIST 识别达到99%。解决了支票、邮编等问题
- GPU 技术发展，2012 1000分类比赛中，冠军超过榜眼10个百分点成绩
> 将要学习：1-卷积神经网络及实现；2-经典卷积神经网络；3-训练神经网络的技巧

## 卷积神经网络及实现；
- 比如一张普通图片800*600像素，这个输入数据量是巨大的RGB情况下，144万像素
- 模糊化是如何操作的的？
- 3*3矩阵定义成 Kernel 称为“核”，称核和矩阵作用的过程为“卷积”；
    - kernel 卷积核
    - 卷积核大小 **kernel size**
    - 横向或纵向移动大小 步长**stride**
- 9个元素 x 9个kernel的元素，再相加得到一个值
- 反复移动，得到一个新的矩阵
- 矩阵外边部分可以填充0 ，保证卷积完了的大小和原图一样大，保留边缘信息，下层可以用边缘信息；另一方面保证满足下层要求输入要求；补零的层数称为**padding**
> 重要公式： Wout =（ W-K+2P ）/ S + 1  
> 若卷积核里的数字不固定，通过监督学习自动找到适合问题的数值——**可以学习的卷积操作**

### **卷积层**
- 权重（矩阵）和偏置（向量）作为参数——合并为隐藏层，也称为卷积层（可以调参）
- 多核多输出（因参数不一样可以多核）
- 卷积操作后添加激活函数（Relu、Sigmoid、Tanh等）
- 因为激活，输出大小在不断变化

### **池化**
- 一般有——最大池化 or 平均池化
- 池化不需要参数
- 统计的统计信息
- 理解为固定大小”池化核“，来回移动获得统计信息
- 作用：
    - 对一个输入进行下采样操作，快速减小输入大小，从而减小神经网络后边的参数量
    - 并能够保持输入原有信息，相对于卷积下的采样，有”不需要参数“的优点

### 总结：
- 就是 卷积层 + 池化层 ，反复出现

### **PyTorch中如何实现** 
若是忘了详见网盘资料

- [torch.nn.Conv2d 卷积函数](https://pytorch.org/docs/stable/nn.html#conv2d)
    - 1.输入通道数
    - 2.输出通道数
    - 3.卷积核大小
    - 4.卷积核移动步长
    - 5.补0的多少
- [nn.MaxPool2d 池化设计](https://pytorch.org/docs/stable/nn.html#maxpool2d)
    - 1.池化核大小；
    - 2.池化核移动步长
    - 3.补多少个0

## 经典卷积神经网络；
- AlexNet
- VGG
### GoogleNet (1个感受器包括四个并行结构分支)
- 1x1卷积——为了提取感受
- 1x1卷积+3x3卷积——减少通道数-3x3扩大感受域
- 1x1卷积+5x5卷积——减少通道数-5x5扩大感受域
- 3x3最大值池化+1x1卷积——先max一遍，与第一分支特征相同相同，输出不同
```python
class inception(nn.Module):
    def __init__(self, in_channel, out1_1, out2_1, out2_3, out3_1, out3_5, out4_1):
        super(inception, self).__init__()
        # 第一条线路
        self.branch1x1 = conv_relu(in_channel, out1_1, 1)
        
        # 第二条线路
        self.branch3x3 = nn.Sequential( 
            conv_relu(in_channel, out2_1, 1),
            conv_relu(out2_1, out2_3, 3, padding=1)
        )
        
        # 第三条线路
        self.branch5x5 = nn.Sequential(
            conv_relu(in_channel, out3_1, 1),
            conv_relu(out3_1, out3_5, 5, padding=2)
        )
        
        # 第四条线路
        self.branch_pool = nn.Sequential(
            nn.MaxPool2d(3, stride=1, padding=1),
            conv_relu(in_channel, out4_1, 1)
        )
        
    def forward(self, x):
        f1 = self.branch1x1(x)
        f2 = self.branch3x3(x)
        f3 = self.branch5x5(x)
        f4 = self.branch_pool(x)
        output = torch.cat((f1, f2, f3, f4), dim=1)
        return output

class googlenet(nn.Module):
    def __init__(self, in_channel, num_classes, verbose=False):
        super(googlenet, self).__init__()
        self.verbose = verbose
        
        self.block1 = nn.Sequential(
            conv_relu(in_channel, out_channel=64, kernel=7, stride=2, padding=3),
            nn.MaxPool2d(3, 2)
        )
        
        self.block2 = nn.Sequential(
            conv_relu(64, 64, kernel=1),
            conv_relu(64, 192, kernel=3, padding=1),
            nn.MaxPool2d(3, 2)
        )
        
        # 反复叠加函数
        self.block3 = nn.Sequential(
            inception(192, 64, 96, 128, 16, 32, 32),
            inception(256, 128, 128, 192, 32, 96, 64),
            nn.MaxPool2d(3, 2)
        )
        
        # 5个inception
        self.block4 = nn.Sequential(
            inception(480, 192, 96, 208, 16, 48, 64),
            inception(512, 160, 112, 224, 24, 64, 64),
            inception(512, 128, 128, 256, 24, 64, 64),
            inception(512, 112, 144, 288, 32, 64, 64),
            inception(528, 256, 160, 320, 32, 128, 128),
            nn.MaxPool2d(3, 2)
        )
        
        self.block5 = nn.Sequential(
            inception(832, 256, 160, 320, 32, 128, 128),
            inception(832, 384, 182, 384, 48, 128, 128),
            nn.AvgPool2d(2)
        )
        
        self.classifier = nn.Linear(1024, num_classes)

    # 把输入向前传    
    def forward(self, x):
        x = self.block1(x)
        if self.verbose:
            print('block 1 output: {}'.format(x.shape))
        x = self.block2(x)
        if self.verbose:
            print('block 2 output: {}'.format(x.shape))
        x = self.block3(x)
        if self.verbose:
            print('block 3 output: {}'.format(x.shape))
        x = self.block4(x)
        if self.verbose:
            print('block 4 output: {}'.format(x.shape))
        x = self.block5(x)
        if self.verbose:
            print('block 5 output: {}'.format(x.shape))
        x = x.view(x.shape[0], -1)
        x = self.classifier(x)
        return x
```
### ResNet
- 跨层链接，解决了部分梯度非常小不被修正的问题
- 对比Google8层，VGG19层， 这个可以做到152层甚至上千层
```python
class residual_block(nn.module):
  def __init__(self,in_channel,out_channel,same_shape=True):
		super(residual_block,self).__init__()
		self.same_shape = same_shape
		stride=1 if self.same_shape else 2
		
		self.conv1 = conv3x3(in_channel,out_channel,stride)
		self.bn1 = nn.BatchNorm2d(out_channel)
		self.conv2 = conv3x3(out_channel,out_channle)
		
		self.bn2 = nn.BatchNorm2d(out_channel)
		if not self.same_shape:
			self.conv3 = nn.Conv2d(in_channel, out_channel, 1, stride=stride)
        
```
### DenseNet
是ResNet的变形
定义一个卷积块，这个卷积块的顺序是 bn -> relu -> conv
```python
def conv_block(in_channel, out_channel):
    layer = nn.Sequential(
        nn.BatchNorm2d(in_channel),
        nn.ReLU(True),
        nn.Conv2d(in_channel, out_channel, 3, padding=1, bias=False)
    )
    return layer
```
dense block 将每次的卷积的输出称为 growth_rate，因为如果输入是 in_channel，有 n 层，那么输出就是 in_channel + n * growh_rate

```python
class dense_block(nn.Module):
    def __init__(self, in_channel, growth_rate, num_layers):
        super(dense_block, self).__init__()
        block = []
        channel = in_channel
        for i in range(num_layers):
            block.append(conv_block(channel, growth_rate))
            channel += growth_rate
            
        self.net = nn.Sequential(*block)
        
    def forward(self, x):
        for layer in self.net:
            out = layer(x)
            x = torch.cat((out, x), dim=1)
        return x
```
我们验证一下输出的 channel 是否正确
```python
test_net = dense_block(3, 12, 3)
test_x = Variable(torch.zeros(1, 3, 96, 96))
print('input shape: {} x {} x {}'.format(test_x.shape[1], test_x.shape[2], test_x.shape[3]))
test_y = test_net(test_x)
print('output shape: {} x {} x {}'.format(test_y.shape[1], test_y.shape[2], test_y.shape[3]))
```
除了 dense block，DenseNet 中还有一个模块叫过渡层（transition block），因为 DenseNet 会不断地对维度进行拼接， 所以当层数很高的时候，输出的通道数就会越来越大，参数和计算量也会越来越大，为了避免这个问题，需要引入过渡层将输出通道降低下来，同时也将输入的长宽减半，这个过渡层可以使用 1 x 1 的卷积
```python
def transition(in_channel, out_channel):
    trans_layer = nn.Sequential(
        nn.BatchNorm2d(in_channel),
        nn.ReLU(True),
        nn.Conv2d(in_channel, out_channel, 1),
        nn.AvgPool2d(2, 2)
    )
    return trans_layer
```
定义 DenseNet
```python
class densenet(nn.Module):
    def __init__(self, in_channel, num_classes, growth_rate=32, block_layers=[6, 12, 24, 16]):
        super(densenet, self).__init__()
        self.block1 = nn.Sequential(
            nn.Conv2d(in_channel, 64, 7, 2, 3),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            nn.MaxPool2d(3, 2, padding=1)
        )
        
        channels = 64
        block = []
        for i, layers in enumerate(block_layers):
            block.append(dense_block(channels, growth_rate, layers))
            channels += layers * growth_rate
            if i != len(block_layers) - 1:
                block.append(transition(channels, channels // 2)) # 通过 transition 层将大小减半，通道数减半
                channels = channels // 2
        
        self.block2 = nn.Sequential(*block)
        self.block2.add_module('bn', nn.BatchNorm2d(channels))
        self.block2.add_module('relu', nn.ReLU(True))
        self.block2.add_module('avg_pool', nn.AvgPool2d(3))
        
        self.classifier = nn.Linear(channels, num_classes)
    
    def forward(self, x):
        x = self.block1(x)
        x = self.block2(x)
        
        x = x.view(x.shape[0], -1)
        x = self.classifier(x)
        return x
```


19/10/27,23:09
