# 07 训练过程"奇淫巧技"(二)学习率衰减
如果学习率固定,那么在梯度的最低点附近时,将会按照学习率不变的情况反复越过最低点,因而达不到梯度最低处.[查看官方文档](https://pytorch.org/docs/0.3.0/optim.html#how-to-adjust-learning-rate)
而一开始学习率就小的话,那么到达最优解位置又比较慢,
因此想定义:**学习率先开始较大,后续慢慢变小**
```python
net= balabalanet(3,10) #这行举例子有个训练网
optimizer = torch.optim.SGD(net.parameters(), lr=0.01, weight_decay=1e-4)
# 们可以通过修改这个属性来改变我们训练过程中的学习率，非常简单
optimizer.param_groups[0]['lr'] = 1e-5
```
为了防止有多个参数组，我们可以使用一个循环-方法2
```python
for param_group in optimizer.param_groups:
    param_group['lr'] = 1e-1
```
