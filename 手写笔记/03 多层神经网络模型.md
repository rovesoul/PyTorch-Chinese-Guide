# 03 多层神经网络模型
- 多个输入x 到 线性方程 加成 Sigmoid→ y-hat
- 多层就是多个回归模型
- hidden layer 就是 linear + sigmoid 
- [激活函数](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions)：[Sigmoid](https://pytorch.org/docs/stable/nn.html#sigmoid)、[ReLU](https://pytorch.org/docs/stable/nn.html#relu)、[Softmax](https://pytorch.org/docs/stable/nn.html#softmax)、SELU……
    - 为什么需要呢？
    - 因为比如多层不加激活函数，等价于一层的，没意义了
> 简单方法：Sequential  和 module
- Sequential
```
seq_net = nn.Sequential(
    nn.Linear(2, 4), # PyTorch中的线性层，wx+b,表输入2维，输出4维
    nn.Tanh(),       # 表激活函数
    nn.Linear(4, 1)  # 表输入4维度，输出1维度
)
```

- Module

看似比Sequential更加复杂，方式更灵活，因为有forward这层

```
class module_net(nn.Module):
    def __init__(self, num_input, num_hidden, num_output):
        super(module_net, self).__init__()
        self.layer1 = nn.Linear(num_input, num_hidden)
        
        self.layer2 = nn.Tanh()
        
        self.layer3 = nn.Linear(num_hidden, num_output)
        
    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        return x
```
