# 09 è®­ç»ƒè¿‡ç¨‹"å¥‡æ·«å·§æŠ€"(å››)æ­£åˆ™åŒ–

$$f = loss + \lambda \sum peparams||p||^2 $$ 

loss --æ™®é€šloss
ğœ† --æƒé‡è¡°å‡å› å­
âˆ‘ --é‡Œè¾¹æ˜¯æƒé‡L2æƒ©ç½š

åœ¨ä¼˜åŒ–ç›®æ ‡æ—¶å€™,ä¸€èˆ¬ä½¿ç”¨L2 æ­£åˆ™åŒ– ï¼Œæ›´æ–°å‚æ•°çš„å…¬å¼å°±æ˜¯

$$
p_j \rightarrow p_j - \eta (\frac{\partial loss}{\partial p_j} + 2 \lambda p_j) = p_j - \eta \frac{\partial loss}{\partial p_j} - 2 \eta \lambda p_j 
$$

å¦‚æœæƒ³åœ¨éšæœºæ¢¯åº¦ä¸‹é™æ³•ä¸­ä½¿ç”¨æ­£åˆ™é¡¹ï¼Œæˆ–è€…è¯´æƒé‡è¡°å‡ï¼Œ`torch.optim.SGD(net.parameters(), lr=0.1, weight_decay=1e-4)` å°±å¯ä»¥äº†ï¼Œè¿™ä¸ª `weight_decay` ç³»æ•°å°±æ˜¯ä¸Šé¢å…¬å¼ä¸­çš„ $\lambda$ï¼Œéå¸¸æ–¹ä¾¿
```python
optimizer = torch.optim.SGD(net.parameters(), lr=0.01, weight_decay=1e-4) 
# weight_decayå°±æ˜¯å¢åŠ æ­£åˆ™é¡¹
```

å…¬å¼å¯èƒ½å‡ºä¸æ¥,ä¸è¿‡ä¸é‡è¦
