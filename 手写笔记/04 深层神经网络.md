# 04 深层神经网络
> MNIST 手写字案例
### 深度神经网络方式
- 10分类，Sigmoid函数不能再保证概率和为1
- softmax(zi)=e^zi / sum (e^zj) 
- softmax 把 n个输出，变成n个概率，其中最大的就是我们要确认的分类
- Loss函数需要适用任何多分类——**交叉熵**
    - 衡量pq相似度：CrossEntropy(p,q)=-1/m sum(p(x)*logq(x))
    - 二分类loss CE= -1/n sum(yi*log*y_hat + (1-yi)log(1-y_hat))
    - [pytorch内置了交叉熵-点击查看](https://pytorch.org/docs/stable/nn.html#crossentropyloss)
```
# 使用内置函数下载 mnist 数据集
train_set = mnist.MNIST('./data', train=True, download=True)
test_set = mnist.MNIST('./data', train=False, download=True)

# 设置训练集、测试集
train_data= DataLoader(train_set, batch_size=64, shuffle=True)
test_data= DataLoader(test_set, batch_size=128, shuffle=False)

# 适用Sequential 定义4层神经网络
net = nn.Sequential(
    nn.Linear(784,400),
    nn.ReLU,
    nn.Linear(400,200),
    nn.ReLU,
    nn.Linear(200,100),
    nn.ReLU,
    nn.Linear(100,10),
)

# 定义 loss 函数
criterion = nn.CrossEntropyLoss() #criterion是自己取的
optimizer = torch.optim.SGD(net.parameters()  # 变量名自己取的
```

## 反向传播算法
- 简单线性模型梯度= ∂loss / ∂ω
- 多层神经网络 =？？计算量太大且很不容易操作
    - 第一步：从输入计算神经网络——每层计算梯度
    - 第二步：从右往左，反向计算梯度
    - 第三步：修正每个节点梯度
    - 第四步：循环上边过程（导数的链式法则）
- 其他优化算法：
    - torch.optim.SGD(param)      #随机梯度下降
    - torch.optim.SGD(param momentum=0.9)  #基于动量梯度下降
    - torch.optim.Adagrad(param)  #自适应梯度下降
    - torch.optim.RMSprop(param)  #RMSprop方法
    - torch.optim.Adadelta(param) #Adadelta方法
    - torch.optim.Adam(param)     #Adam方法
