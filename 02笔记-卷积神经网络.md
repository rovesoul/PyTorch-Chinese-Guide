# 卷积神经网络
> 上个笔记中了解了 分类问题，回归问题，学习了多层神经网络结构的样子。还学习了反向传播算法等。但现实生活中很多问题他们解决不了。在神经网络优化过程中，出现了卷积神经网络。大体记录的一些发展节点如下：
- 上世纪机器识图，如手写邮编如何识别呢？
- 上世纪60年代观察猫，发现视觉神经有一小部分的响应，引起思考
- 上世纪80年代反向传播，卷积神经网络开始流行，
- 1998年，LeCun提出LeNet-5MNIST 识别达到99%。解决了支票、邮编等问题
- GPU 技术发展，2012 1000分类比赛中，冠军超过榜眼10个百分点成绩
> 将要学习：1-卷积神经网络及实现；2-经典卷积神经网络；3-训练神经网络的技巧

## 卷积神经网络及实现；
- 比如一张普通图片800*600像素，这个输入数据量是巨大的RGB情况下，144万像素
- 模糊化是如何操作的的？
- 3*3矩阵定义成 Kernel 称为“核”，称核和矩阵作用的过程为“卷积”；
    - kernel 卷积核
    - 卷积核大小 **kernel size**
    - 横向或纵向移动大小 步长**stride**
- 9个元素 x 9个kernel的元素，再相加得到一个值
- 反复移动，得到一个新的矩阵
- 矩阵外边部分可以填充0 ，保证卷积完了的大小和原图一样大，保留边缘信息，下层可以用边缘信息；另一方面保证满足下层要求输入要求；补零的层数称为**padding**
> 重要公式： Wout =（ W-K+2P ）/ S + 1  
> 若卷积核里的数字不固定，通过监督学习自动找到适合问题的数值——**可以学习的卷积操作**

### **卷积层**
- 权重（矩阵）和偏置（向量）作为参数——合并为隐藏层，也称为卷积层（可以调参）
- 多核多输出（因参数不一样可以多核）
- 卷积操作后添加激活函数（Relu、Sigmoid、Tanh等）
- 因为激活，输出大小在不断变化

### **池化**
- 一般有——最大池化 or 平均池化
- 池化不需要参数
- 统计的统计信息
- 理解为固定大小”池化核“，来回移动获得统计信息
- 作用：
    - 对一个输入进行下采样操作，快速减小输入大小，从而减小神经网络后边的参数量
    - 并能够保持输入原有信息，相对于卷积下的采样，有”不需要参数“的优点

### 总结：
- 就是 卷积层 + 池化层 ，反复出现

### **PyTorch中如何实现** 
若是忘了详见网盘资料

- [torch.nn.Conv2d 卷积函数](https://pytorch.org/docs/stable/nn.html#conv2d)
    - 1.输入通道数
    - 2.输出通道数
    - 3.卷积核大小
    - 4.卷积核移动步长
    - 5.补0的多少
- [nn.MaxPool2d 池化设计](https://pytorch.org/docs/stable/nn.html#maxpool2d)
    - 1.池化核大小；
    - 2.池化核移动步长
    - 3.补多少个0

## 经典卷积神经网络；
- CIFAR10
- AlexNet
- VGG
- GoogleNet
- ResNet
- DenseNet


19/10/27,02:09